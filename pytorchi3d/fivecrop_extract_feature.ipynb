{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mp4 to jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert mp4 to jpg : Cold.Eyes_0-00-04_0-00-13_0-00-20_0-00-31_0-01-02_0-01-04_0-01_21_0-01-24_label_P\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from utils import createDirectory\n",
    "\n",
    "# jpg로 변환할 비디오들이 담긴 폴더 지정\n",
    "video_root_path = '/opt/ml/input/data/videos'\n",
    "\n",
    "# 저장되는 폴더 경로 지정\n",
    "image_root_path = '/opt/ml/input/data/images'\n",
    "createDirectory(image_root_path)\n",
    "\n",
    "video_list = [os.path.join(video_root_path, i) for i in os.listdir(video_root_path)]\n",
    "image_list = [os.path.join(image_root_path, i) for i in os.listdir(video_root_path)]\n",
    "\n",
    "for file_path, save_path in zip(video_list, image_list):\n",
    "    createDirectory(save_path[:-4])\n",
    "\n",
    "    start = file_path.find('/', file_path.find('/', file_path.find('/', file_path.find('/', file_path.find('/', file_path.find('/') + 1)+1)+1)+1)+1)\n",
    "\n",
    "    video_name = file_path[start+1:-4]\n",
    "    print('convert mp4 to jpg :', video_name)\n",
    "\n",
    "    cap = cv2.VideoCapture(file_path)\n",
    "    FPS = cap.get(5) \n",
    "    frame = cap.get(7)\n",
    "    count = 1\n",
    "\n",
    "    if FPS == 30:\n",
    "        for i in range(1, int(frame)+1):\n",
    "            success, img = cap.read()\n",
    "            \n",
    "            if not success:\n",
    "                break\n",
    "\n",
    "            if i % (1.25) < 1:\n",
    "                cv2.imwrite(file_path[:-4] + f\"/{video_name}\"+'-'+str(count).zfill(6)+'.jpg', img)\n",
    "                count+=1\n",
    "    elif FPS == 24:\n",
    "        for i in range(1, int(frame)+1):\n",
    "            success, img = cap.read()\n",
    "\n",
    "            if not success:\n",
    "                break\n",
    "\n",
    "            height, width, channel = img.shape\n",
    "            \n",
    "            # if width > 640:\n",
    "            #     d = width/640\n",
    "            #     sc = 1/d\n",
    "            #     img = cv2.resize(img,dsize=(0,0),fx=sc,fy=sc)\n",
    "\n",
    "            img = cv2.resize(img, (640, 360))\n",
    "\n",
    "            cv2.imwrite(save_path[:-4] + f\"/{video_name}\"+'-'+str(count).zfill(6)+'.jpg', img)\n",
    "            count+=1\n",
    "    else:\n",
    "        print(\"*\"*30)\n",
    "        print(f\"FPS : {FPS} - {file_path}\")\n",
    "        print(\"*\"*30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import videotransforms\n",
    "\n",
    "import numpy as np\n",
    "from pytorch_i3d import InceptionI3d\n",
    "from charades_dataset_full import Charades as Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms = transforms.Compose([])\n",
    "first_transforms = transforms.Compose([videotransforms.FirstCrop(224)])\n",
    "second_transforms = transforms.Compose([videotransforms.SecondCrop(224)])\n",
    "third_transforms = transforms.Compose([videotransforms.ThirdCrop(224)])\n",
    "fourth_transforms = transforms.Compose([videotransforms.FourthCrop(224)])\n",
    "center_transforms = transforms.Compose([videotransforms.CenterCrop(224)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/ml/input/data/image_features/Cold.Eyes_0-00-04_0-00-13_0-00-20_0-00-31_0-01-02_0-01-04_0-01_21_0-01-24_label_P__0.npy\n",
      "/opt/ml/input/data/image_features/Cold.Eyes_0-00-04_0-00-13_0-00-20_0-00-31_0-01-02_0-01-04_0-01_21_0-01-24_label_P__1.npy\n",
      "/opt/ml/input/data/image_features/Cold.Eyes_0-00-04_0-00-13_0-00-20_0-00-31_0-01-02_0-01-04_0-01_21_0-01-24_label_P__2.npy\n",
      "/opt/ml/input/data/image_features/Cold.Eyes_0-00-04_0-00-13_0-00-20_0-00-31_0-01-02_0-01-04_0-01_21_0-01-24_label_P__3.npy\n",
      "/opt/ml/input/data/image_features/Cold.Eyes_0-00-04_0-00-13_0-00-20_0-00-31_0-01-02_0-01-04_0-01_21_0-01-24_label_P__4.npy\n"
     ]
    }
   ],
   "source": [
    "root = '/opt/ml/input/data/'\n",
    "mode = 'rgb'\n",
    "batch_size = 1\n",
    "save_dir = '/opt/ml/input/data/image_features'\n",
    "load_model = '/opt/ml/input/code/project/pytorch-i3d/models/rgb_imagenet.pt'\n",
    "\n",
    "first_dataset = Dataset(root, mode, first_transforms, num=-1, save_dir=save_dir)\n",
    "first_dataloader = torch.utils.data.DataLoader(first_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "second_dataset = Dataset(root, mode, second_transforms, num=-1, save_dir=save_dir)\n",
    "second_dataloader = torch.utils.data.DataLoader(second_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "third_dataset = Dataset(root, mode, third_transforms, num=-1, save_dir=save_dir)\n",
    "third_dataloader = torch.utils.data.DataLoader(third_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "fourth_dataset = Dataset(root, mode, fourth_transforms, num=-1, save_dir=save_dir)\n",
    "fourth_dataloader = torch.utils.data.DataLoader(fourth_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "center_dataset = Dataset(root, mode, center_transforms, num=-1, save_dir=save_dir)\n",
    "center_dataloader = torch.utils.data.DataLoader(center_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "# center_dataloader : center|\n",
    "# first_dataloader : top-left\n",
    "# second_dataloader : top-right\n",
    "# third_dataloader : bottom-right\n",
    "# fourth_dataloader : bottom-left\n",
    "\n",
    "dataloaders = {\n",
    "               '0': center_dataloader,\n",
    "               '1': first_dataloader,\n",
    "               '2': second_dataloader,\n",
    "               '4': third_dataloader,\n",
    "               '3': fourth_dataloader,\n",
    "                }\n",
    "\n",
    "datasets = {\n",
    "            '0': center_dataset,\n",
    "            '1': first_dataset,\n",
    "            '2': second_dataset,\n",
    "            '4': third_dataset,\n",
    "            '3': fourth_dataset,\n",
    "            }  \n",
    "\n",
    "i3d = InceptionI3d(400, in_channels=3)\n",
    "i3d.replace_logits(400)\n",
    "i3d.load_state_dict(torch.load(load_model))\n",
    "i3d.cuda()\n",
    "i3d.train(False)  # Set model to evaluate mode\n",
    "        \n",
    "# Iterate over data.\n",
    "saved_list = []\n",
    "for phase in ['0', '1', '2', '3', '4']:\n",
    "    for data in dataloaders[phase]:\n",
    "        # get the inputs\n",
    "        inputs, labels, name = data\n",
    "        b,c,t,h,w = inputs.shape\n",
    "        features = []\n",
    "        for start in range(0, t, 16): \n",
    "            end = start + 16 \n",
    "            ip = Variable(torch.from_numpy(inputs.numpy()[:,:,start:end]).cuda())\n",
    "            if ip.shape[2] != 16:\n",
    "                continue\n",
    "            features.append(i3d.extract_features(ip).squeeze(0).permute(1,2,3,0).data.cpu().numpy())\n",
    "        np.save(os.path.join(save_dir, name[0]+f\"__{phase}\"), np.concatenate(features, axis=0).reshape(-1, 1024))\n",
    "        saved_list.append(f\"{os.path.join(save_dir, name[0]+'__'+phase)}.npy\")\n",
    "        print(f\"{os.path.join(save_dir, name[0]+'__'+phase)}.npy\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold.Eyes_0-00-04_0-00-13_0-00-20_0-00-31_0-01-02_0-01-04_0-01_21_0-01-24_label_P__0.npy - (139, 1024)\n",
      "Cold.Eyes_0-00-04_0-00-13_0-00-20_0-00-31_0-01-02_0-01-04_0-01_21_0-01-24_label_P__1.npy - (139, 1024)\n",
      "Cold.Eyes_0-00-04_0-00-13_0-00-20_0-00-31_0-01-02_0-01-04_0-01_21_0-01-24_label_P__2.npy - (139, 1024)\n",
      "Cold.Eyes_0-00-04_0-00-13_0-00-20_0-00-31_0-01-02_0-01-04_0-01_21_0-01-24_label_P__3.npy - (139, 1024)\n",
      "Cold.Eyes_0-00-04_0-00-13_0-00-20_0-00-31_0-01-02_0-01-04_0-01_21_0-01-24_label_P__4.npy - (139, 1024)\n"
     ]
    }
   ],
   "source": [
    "for sample in saved_list:\n",
    "    data = np.load(sample)\n",
    "    print(f'{sample[len(save_dir)+1:]} - {data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
