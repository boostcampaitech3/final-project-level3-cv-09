{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from utils import createDirectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mp4 to jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = '/opt/ml/input/data/A.Beautiful.Mind.2001__#00-25-20_00-29-20_label_A.mp4'\n",
    "createDirectory(file_path[:-4])\n",
    "\n",
    "start = file_path.find('/', file_path.find('/', file_path.find('/', file_path.find('/') + 1)+1)+1)\n",
    "\n",
    "video_name = file_path[start+1:-4]\n",
    "\n",
    "cap = cv2.VideoCapture(file_path)\n",
    "FPS = cap.get(5) \n",
    "frame = cap.get(7)\n",
    "count = 1\n",
    "\n",
    "if FPS == 30:\n",
    "    for i in range(1, int(frame)):\n",
    "        success, img = cap.read()\n",
    "        \n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        if i % (1.25) < 1:\n",
    "            cv2.imwrite(file_path[:-4] + f\"/{video_name}\"+'-'+str(count).zfill(6)+'.jpg', img)\n",
    "            count+=1\n",
    "elif FPS == 24:\n",
    "    # for i in range(1, int(frame)):\n",
    "    for i in range(1, 50):\n",
    "        success, img = cap.read()\n",
    "        \n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        cv2.imwrite(file_path[:-4] + f\"/{video_name}\"+'-'+str(count).zfill(6)+'.jpg', img)\n",
    "        count+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import videotransforms\n",
    "\n",
    "import numpy as np\n",
    "from pytorch_i3d import InceptionI3d\n",
    "from charades_dataset_full import Charades as Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms = transforms.Compose([])\n",
    "first_transforms = transforms.Compose([videotransforms.FirstCrop(224)])\n",
    "second_transforms = transforms.Compose([videotransforms.SecondCrop(224)])\n",
    "third_transforms = transforms.Compose([videotransforms.ThirdCrop(224)])\n",
    "fourth_transforms = transforms.Compose([videotransforms.FourthCrop(224)])\n",
    "center_transforms = transforms.Compose([videotransforms.CenterCrop(224)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Saved : /opt/ml/input/data/A.Beautiful.Mind.2001__#00-25-20_00-29-20_label_A__0.npy\n",
      "Success Saved : /opt/ml/input/data/A.Beautiful.Mind.2001__#00-25-20_00-29-20_label_A__1.npy\n",
      "Success Saved : /opt/ml/input/data/A.Beautiful.Mind.2001__#00-25-20_00-29-20_label_A__2.npy\n",
      "Success Saved : /opt/ml/input/data/A.Beautiful.Mind.2001__#00-25-20_00-29-20_label_A__3.npy\n",
      "Success Saved : /opt/ml/input/data/A.Beautiful.Mind.2001__#00-25-20_00-29-20_label_A__4.npy\n"
     ]
    }
   ],
   "source": [
    "split = '/opt/ml/input/data/test.json'\n",
    "root = '/opt/ml/input/data'\n",
    "mode = 'rgb'\n",
    "batch_size = 1\n",
    "save_dir = '/opt/ml/input/data'\n",
    "load_model = '/opt/ml/input/code/project/pytorch-i3d/models/rgb_imagenet.pt'\n",
    "\n",
    "first_dataset = Dataset(split, 'testing', root, mode, first_transforms, num=-1, save_dir=save_dir)\n",
    "first_dataloader = torch.utils.data.DataLoader(first_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "second_dataset = Dataset(split, 'testing', root, mode, second_transforms, num=-1, save_dir=save_dir)\n",
    "second_dataloader = torch.utils.data.DataLoader(second_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "third_dataset = Dataset(split, 'testing', root, mode, third_transforms, num=-1, save_dir=save_dir)\n",
    "third_dataloader = torch.utils.data.DataLoader(third_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "fourth_dataset = Dataset(split, 'testing', root, mode, fourth_transforms, num=-1, save_dir=save_dir)\n",
    "fourth_dataloader = torch.utils.data.DataLoader(fourth_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "center_dataset = Dataset(split, 'testing', root, mode, center_transforms, num=-1, save_dir=save_dir)\n",
    "center_dataloader = torch.utils.data.DataLoader(center_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "\n",
    "dataloaders = {\n",
    "               '0': first_dataloader,\n",
    "               '1': second_dataloader,\n",
    "               '2': third_dataloader,\n",
    "               '3': fourth_dataloader,\n",
    "               '4': center_dataloader,\n",
    "                }\n",
    "\n",
    "datasets = {\n",
    "            '0': first_dataset,\n",
    "            '1': second_dataset,\n",
    "            '2': third_dataset,\n",
    "            '3': fourth_dataset,\n",
    "            '4': center_dataset,\n",
    "            }  \n",
    "\n",
    "i3d = InceptionI3d(400, in_channels=3)\n",
    "i3d.replace_logits(400)\n",
    "i3d.load_state_dict(torch.load(load_model))\n",
    "i3d.cuda()\n",
    "i3d.train(False)  # Set model to evaluate mode\n",
    "        \n",
    "# Iterate over data.\n",
    "for phase in ['0', '1', '2', '3', '4']:\n",
    "    for data in dataloaders[phase]:\n",
    "        # get the inputs\n",
    "        inputs, labels, name = data\n",
    "        b,c,t,h,w = inputs.shape\n",
    "        features = []\n",
    "        for start in range(1, t, 16):\n",
    "            end = start + 16 \n",
    "            ip = Variable(torch.from_numpy(inputs.numpy()[:,:,start:end]).cuda())\n",
    "            features.append(i3d.extract_features(ip).squeeze(0).permute(1,2,3,0).data.cpu().numpy())\n",
    "        # np.save(os.path.join(save_dir, name[0]+f\"_{phase}\"), np.concatenate(features, axis=0))\n",
    "        np.save(os.path.join(save_dir, name[0]+f\"__{phase}\"), np.concatenate(features, axis=0).reshape(-1, 1024))\n",
    "        print(f\"Success Saved : {os.path.join(save_dir, name[0]+'__'+phase)}.npy\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_data = np.load('/opt/ml/input/data/test_video_1.npy')\n",
    "second_data = np.load('/opt/ml/input/data/test_video_2.npy')\n",
    "third_data = np.load('/opt/ml/input/data/test_video_3.npy')\n",
    "fourth_data = np.load('/opt/ml/input/data/test_video_4.npy')\n",
    "center_data = np.load('/opt/ml/input/data/test_video_5.npy')\n",
    "\n",
    "print('first_crop :', first_data.shape)\n",
    "print('second_crop :', second_data.shape)\n",
    "print('third_crop :', third_data.shape)\n",
    "print('fourth_crop :', fourth_data.shape)\n",
    "print('center_crop :', center_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_crop : (360, 1024)\n",
      "second_crop : (360, 1024)\n",
      "third_crop : (360, 1024)\n",
      "fourth_crop : (360, 1024)\n",
      "center_crop : (360, 1024)\n"
     ]
    }
   ],
   "source": [
    "first_data = np.load('/opt/ml/input/data/A.Beautiful.Mind.2001__#00-25-20_00-29-20_label_A__0.npy')\n",
    "second_data = np.load('/opt/ml/input/data/A.Beautiful.Mind.2001__#00-25-20_00-29-20_label_A__1.npy')\n",
    "third_data = np.load('/opt/ml/input/data/A.Beautiful.Mind.2001__#00-25-20_00-29-20_label_A__2.npy')\n",
    "fourth_data = np.load('/opt/ml/input/data/A.Beautiful.Mind.2001__#00-25-20_00-29-20_label_A__3.npy')\n",
    "center_data = np.load('/opt/ml/input/data/A.Beautiful.Mind.2001__#00-25-20_00-29-20_label_A__4.npy')\n",
    "\n",
    "print('first_crop :', first_data.shape)\n",
    "print('second_crop :', second_data.shape)\n",
    "print('third_crop :', third_data.shape)\n",
    "print('fourth_crop :', fourth_data.shape)\n",
    "print('center_crop :', center_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_crop : (360, 1024)\n",
      "second_crop : (360, 1024)\n",
      "third_crop : (360, 1024)\n",
      "fourth_crop : (360, 1024)\n",
      "center_crop : (360, 1024)\n"
     ]
    }
   ],
   "source": [
    "first_data_gt = np.load('/opt/ml/input/data/gt/A.Beautiful.Mind.2001__#00-25-20_00-29-20_label_A__0.npy')\n",
    "second_data_gt = np.load('/opt/ml/input/data/gt/A.Beautiful.Mind.2001__#00-25-20_00-29-20_label_A__1.npy')\n",
    "third_data_gt = np.load('/opt/ml/input/data/gt/A.Beautiful.Mind.2001__#00-25-20_00-29-20_label_A__2.npy')\n",
    "fourth_data_gt = np.load('/opt/ml/input/data/gt/A.Beautiful.Mind.2001__#00-25-20_00-29-20_label_A__3.npy')\n",
    "center_data_gt = np.load('/opt/ml/input/data/gt/A.Beautiful.Mind.2001__#00-25-20_00-29-20_label_A__4.npy')\n",
    "\n",
    "print('first_crop :', first_data_gt.shape)\n",
    "print('second_crop :', second_data_gt.shape)\n",
    "print('third_crop :', third_data_gt.shape)\n",
    "print('fourth_crop :', fourth_data_gt.shape)\n",
    "print('center_crop :', center_data_gt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.19726132, 0.05483772, 0.15460849, 0.08884379, 0.2266495 ,\n",
       "        0.11017099, 0.15304732, 0.22752951, 0.13596202, 0.17326353],\n",
       "       dtype=float32),\n",
       " array([0.39029422, 0.1425842 , 0.37637228, 0.25315678, 0.24167438,\n",
       "        0.14017773, 0.1671794 , 0.177893  , 0.18694301, 0.10136556],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_data[0][:10], first_data_gt[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
